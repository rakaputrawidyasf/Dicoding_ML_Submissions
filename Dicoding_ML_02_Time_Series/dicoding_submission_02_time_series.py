# -*- coding: utf-8 -*-
"""dicoding_submission_02_Time_Series.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jv3p8aKYi88gjLwbPGvcfRM7bSGNWceF

# **Dicoding - Belajar Pengembangan Machine Learning**
# Submission 2 - Time Series

### Daily Climate Time-Series Dataset<br>
https://www.kaggle.com/sumanthvrao/daily-climate-time-series-data
"""

!pip install -q kaggle

from google.colab import files

import os
import zipfile

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow import keras
from keras.layers import Dense, LSTM

files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

"""### Download Dataset

"""

!kaggle datasets download sumanthvrao/daily-climate-time-series-data

zip_data = "daily-climate-time-series-data.zip"
zip_ref = zipfile.ZipFile(zip_data, "r")
zip_ref.extractall()
zip_ref.close()

"""### Read & Process CSV Data"""

df = pd.read_csv("DailyDelhiClimateTrain.csv", engine="python")
df

list(df.columns)

df.date = pd.to_datetime(df.date)

"""Select "wind_speed""""

df_new = df.drop(["meantemp", "humidity", "meanpressure"], axis=1)
df_new

"""Display"""

date = df_new["date"].values
wind_speed = df_new["wind_speed"].values

plt.figure(figsize=(12, 6))
plt.grid()
plt.plot(date, wind_speed, color="g")
plt.title("Daily Climate (Wind Speed)", fontsize=16)

"""### Training & Validation"""

X_train, X_test, y_train, y_test = train_test_split(wind_speed, date,
                                                    test_size=0.2,
                                                    random_state=0,
                                                    shuffle=False)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(X_train, window_size=60, batch_size=100, shuffle_buffer=2000)
test_set = windowed_dataset(X_test, window_size=60, batch_size=100, shuffle_buffer=2000)

model = keras.models.Sequential([
    keras.layers.LSTM(64, return_sequences=True),
    keras.layers.LSTM(64, return_sequences=True),
    keras.layers.Dense(32, activation="relu"),
    keras.layers.Dense(16, activation="relu"),
    keras.layers.Dense(1),
    keras.layers.Lambda(lambda x: x * 400)
])

optimizer = keras.optimizers.SGD(learning_rate=1.000e-04, momentum=0.9)

model.compile(loss=keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

X_min = df_new["wind_speed"].min()
X_min

X_max = df_new["wind_speed"].max()
X_max

X_avg = (X_max - X_min) * (10 / 100)
X_avg

class CallBack(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get("mae") < X_avg):
      print("MAE of the model < 10% of data scale. Stop training...")
      self.model.stop_training = True

callbacks = CallBack()

history = model.fit(train_set,
                    epochs=50,
                    validation_data=test_set,
                    callbacks=[callbacks])

plt.plot(history.history["mae"])
plt.plot(history.history["val_mae"])
plt.title("Model MAE")
plt.ylabel("MAE")
plt.xlabel("Epoch")
plt.legend(["Train", "Test"], loc="upper right")
plt.show()

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("Model loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend(["Train", "Test"], loc="upper right")
plt.show()