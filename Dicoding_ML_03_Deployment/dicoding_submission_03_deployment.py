# -*- coding: utf-8 -*-
"""dicoding_submission_03_deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ulTatsFTKEEia7zUEsvzqpIdTa9xQmLN

# **Dicoding - Belajar Pengembangan Machine Learning**
# Submission 3 - Image Classification Model Deployment

### Flowers Recognition<br>
https://www.kaggle.com/alxmamaev/flowers-recognition
"""

!pip install -q kaggle

from google.colab import files

import os
import shutil
import zipfile

import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from keras.preprocessing.image import ImageDataGenerator

files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

"""### Download Dataset"""

!kaggle datasets download -d alxmamaev/flowers-recognition

zip_data = "flowers-recognition.zip"
zip_ref = zipfile.ZipFile(zip_data, "r")
zip_ref.extractall()
zip_ref.close()

"""### Preprocessing the Dataset"""

dataset_dir = os.path.join("flowers")
labels = os.listdir(dataset_dir)

label_and_items = {}

for label in labels:
  items = os.listdir(os.path.join(dataset_dir, label))
  label_and_items[label] = len(items)

label_and_items

def remove_label(label):
  shutil.rmtree(os.path.join(dataset_dir, label))
  labels.remove(label)
  label_and_items.pop(label)

remove_label("dandelion")
remove_label("tulip")

labels

label_and_items

"""### Split into Training and Validation set"""

datagen = ImageDataGenerator(rescale=1./255.0,
                             rotation_range=20,
                             horizontal_flip=True,
                             shear_range=0.2,
                             zoom_range=0.2,
                             fill_mode="nearest",
                             validation_split=0.2)

image_size = (150, 150)

train_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=image_size,
    batch_size=32,
    class_mode="categorical",
    subset="training")
    
validation_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=image_size,
    batch_size=32,
    class_mode="categorical",
    subset="validation")

"""### Sequential Model"""

input_shape = (150, 150, 3)

model = keras.Sequential([
    keras.layers.Conv2D(32, (3, 3), activation="relu",
                        input_shape=input_shape),
    keras.layers.MaxPooling2D(2, 2),
    keras.layers.Conv2D(64, (3, 3), activation="relu"),
    keras.layers.MaxPooling2D(2, 2),
    keras.layers.Conv2D(128, (3, 3), activation="relu"),
    keras.layers.MaxPooling2D(2, 2),
    keras.layers.Flatten(),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(256, activation="relu"),
    keras.layers.Dense(len(labels), activation="softmax")
])

model.compile(optimizer="adam",
              loss="categorical_crossentropy",
              metrics=["accuracy"])

model.summary()

"""### Model Training & Validation"""

class CustomCallBack(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if logs.get("accuracy") >= 0.9000 and logs.get("val_accuracy") >= 0.9000:
      print("Accuracy & Val accuracy was over 85%. Stop training...")
      self.model.stop_training = True

callbacks = CustomCallBack()

history = model.fit(train_generator,
                    epochs=32,
                    validation_data=validation_generator,
                    callbacks=[callbacks])

plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.title("Akurasi Model")
plt.ylabel("accuracy")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("Loss Model")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

"""### Deployment"""

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with tf.io.gfile.GFile("model.tflite", "wb") as f:
  f.write(tflite_model)