# -*- coding: utf-8 -*-
"""dicoding_submission_01_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PLeNrs5NEliWxLpK3jLiIdzTywJD9yDW

# **Dicoding - Belajar Pengembangan Machine Learning**
# Submission 1 - NLP

Dataset:
"""

!pip install -q kaggle

from google.colab import files

import os
import zipfile

import pandas as pd

from sklearn.model_selection import train_test_split

from tensorflow import keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

import matplotlib.pyplot as plt

files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d thomaskonstantin/highly-rated-children-books-and-stories

zip_data = "highly-rated-children-books-and-stories.zip"
zip_ref = zipfile.ZipFile(zip_data, "r")
zip_ref.extractall()
zip_ref.close()

df = pd.read_csv("children_books.csv", engine="python")
df

list(df.columns)

"""Classify "Desc" contents by "Reading_age" categories"""

df.Reading_age.value_counts()

reading_age = pd.get_dummies(df.Reading_age.str.strip())
df_new = pd.concat([df, reading_age], axis=1)
df_new = df_new.drop(columns=["Title", "Author", "Inerest_age", "Reading_age"])
df_new

list(df_new.columns)

# X for "Tweet" values
# y for labels

X = df_new["Desc"].values
y = df_new[["10+", "10-12", "10-14", "11+", "12+", "13", "13+", "14+", "8+", "9+"]].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

tokenizer = Tokenizer(num_words=5000, oov_token="x")
#tokenizer = Tokenizer(num_words=5000, oov_token="x",
                      #filters='!"#$%&()*+,-./:;<=>@[\]^_`{|}~ ')

tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)

train_sequences = tokenizer.texts_to_sequences(X_train)
test_sequences = tokenizer.texts_to_sequences(X_test)

padded_train = pad_sequences(train_sequences)
padded_test = pad_sequences(test_sequences)

model = keras.Sequential([
    keras.layers.Embedding(input_dim=5000, output_dim=64),
    keras.layers.LSTM(64),
    keras.layers.Dense(32, activation="relu"),
    keras.layers.Dense(16, activation="relu"),
    keras.layers.Dense(10, activation="softmax")
])

model.compile(loss="categorical_crossentropy",
              optimizer="adam",
              metrics=["accuracy"])

model.summary()

class CallBack(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get("accuracy")>=0.950):
      print("Accuracy was >= 95%. Stop training...")
      self.model.stop_training = True

callbacks = CallBack()

epochs = 10
history = model.fit(padded_train, y_train,
                    epochs=epochs,
                    validation_data=(padded_test, y_test),
                    verbose=2,
                    callbacks=[callbacks])

plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.title("Model accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.legend(["Train", "Test"], loc="upper right")
plt.show()

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("Model loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend(["Train", "Test"], loc="upper right")
plt.show()